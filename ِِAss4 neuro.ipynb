{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "4c501d13-1608-4531-9cac-e44b5bcd1e94",
      "cell_type": "code",
      "source": "import numpy as np\ntext = \"My name is Menna\"\nwords = text.split()\nvocab = list(set(words))\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\nidx_to_word = {idx: word for word, idx in word_to_idx.items()}\nvocab_size = len(vocab)\n\ninput_sequence = [word_to_idx[word] for word in words[:3]] \ntarget_word = word_to_idx[words[3]]  \n\ndef one_hot_encode(idx, vocab_size):\n    vec = np.zeros(vocab_size)\n    vec[idx] = 1\n    return vec\n\nX = np.array([one_hot_encode(idx, vocab_size) for idx in input_sequence]) \nY = one_hot_encode(target_word, vocab_size)\n\nhidden_size = 5 \nlearning_rate = 0.1\nepochs = 10000\n\nWxh = np.random.randn(hidden_size, vocab_size) * 0.01  \nWhh = np.random.randn(hidden_size, hidden_size) * 0.01  \nWhy = np.random.randn(vocab_size, hidden_size) * 0.01  \nbh = np.zeros((hidden_size, 1))  \nby = np.zeros((vocab_size, 1)) \n\ndef forward(X, Wxh, Whh, Why, bh, by):\n    h = np.zeros((hidden_size, 1))  \n    hs, ys = {}, {}\n    \n    for t in range(len(X)):\n        xt = X[t].reshape(-1, 1)  \n        h = np.tanh(np.dot(Wxh, xt) + np.dot(Whh, h) + bh)\n        hs[t] = h\n        \n    y = np.dot(Why, h) + by\n    ys[len(X)] = y  \n    \n    return hs, ys\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))  \n    return exp_x / np.sum(exp_x)\n\ndef loss(Y, Y_pred):\n    return -np.sum(Y * np.log(Y_pred))\n\ndef backward(X, Y, hs, ys, Wxh, Whh, Why, bh, by):\n    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n    dh_next = np.zeros_like(hs[0])\n    \n    dy = ys[len(X)] - Y.reshape(-1, 1)\n    dWhy += np.dot(dy, hs[len(X)-1].T)\n    dby += dy\n    \n    for t in reversed(range(len(X))):\n        dh = np.dot(Why.T, dy) + dh_next\n        dh_raw = (1 - hs[t] ** 2) * dh  # Derivative of tanh\n        dbh += dh_raw\n        dWxh += np.dot(dh_raw, X[t].reshape(1, -1))\n        dWhh += np.dot(dh_raw, hs[t-1].T if t > 0 else np.zeros_like(hs[0]).T)\n        dh_next = np.dot(Whh.T, dh_raw)\n    \n    return dWxh, dWhh, dWhy, dbh, dby\n\nfor epoch in range(epochs):\n    hs, ys = forward(X, Wxh, Whh, Why, bh, by)\n    Y_pred = softmax(ys[len(X)].flatten())\n    \n    current_loss = loss(Y, Y_pred)\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Loss: {current_loss}\")\n    \n    dWxh, dWhh, dWhy, dbh, dby = backward(X, Y, hs, ys, Wxh, Whh, Why, bh, by)\n    \n    Wxh -= learning_rate * dWxh\n    Whh -= learning_rate * dWhh\n    Why -= learning_rate * dWhy\n    bh -= learning_rate * dbh\n    by -= learning_rate * dby\n\ndef predict(X, Wxh, Whh, Why, bh, by):\n    _, ys = forward(X, Wxh, Whh, Why, bh, by)\n    Y_pred = softmax(ys[len(X)].flatten())\n    predicted_idx = np.argmax(Y_pred)\n    return idx_to_word[predicted_idx]\n\npredicted_word = predict(X, Wxh, Whh, Why, bh, by)\nprint(f\"Predicted 4th word: {predicted_word}\")",
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 0, Loss: 1.3859201253156088\nEpoch 100, Loss: 0.7436763741783347\nEpoch 200, Loss: 0.7436683807364797\nEpoch 300, Loss: 0.7436683806286807\nEpoch 400, Loss: 0.7436683806286791\nEpoch 500, Loss: 0.7436683806286791\nEpoch 600, Loss: 0.7436683806286791\nEpoch 700, Loss: 0.7436683806286791\nEpoch 800, Loss: 0.7436683806286791\nEpoch 900, Loss: 0.7436683806286791\nEpoch 1000, Loss: 0.7436683806286791\nEpoch 1100, Loss: 0.7436683806286791\nEpoch 1200, Loss: 0.7436683806286791\nEpoch 1300, Loss: 0.7436683806286791\nEpoch 1400, Loss: 0.7436683806286791\nEpoch 1500, Loss: 0.7436683806286791\nEpoch 1600, Loss: 0.7436683806286791\nEpoch 1700, Loss: 0.7436683806286791\nEpoch 1800, Loss: 0.7436683806286791\nEpoch 1900, Loss: 0.7436683806286791\nEpoch 2000, Loss: 0.7436683806286791\nEpoch 2100, Loss: 0.7436683806286791\nEpoch 2200, Loss: 0.7436683806286791\nEpoch 2300, Loss: 0.7436683806286791\nEpoch 2400, Loss: 0.7436683806286791\nEpoch 2500, Loss: 0.7436683806286791\nEpoch 2600, Loss: 0.7436683806286791\nEpoch 2700, Loss: 0.7436683806286791\nEpoch 2800, Loss: 0.7436683806286791\nEpoch 2900, Loss: 0.7436683806286791\nEpoch 3000, Loss: 0.7436683806286791\nEpoch 3100, Loss: 0.7436683806286791\nEpoch 3200, Loss: 0.7436683806286791\nEpoch 3300, Loss: 0.7436683806286791\nEpoch 3400, Loss: 0.7436683806286791\nEpoch 3500, Loss: 0.7436683806286791\nEpoch 3600, Loss: 0.7436683806286791\nEpoch 3700, Loss: 0.7436683806286791\nEpoch 3800, Loss: 0.7436683806286791\nEpoch 3900, Loss: 0.7436683806286791\nEpoch 4000, Loss: 0.7436683806286791\nEpoch 4100, Loss: 0.7436683806286791\nEpoch 4200, Loss: 0.7436683806286791\nEpoch 4300, Loss: 0.7436683806286791\nEpoch 4400, Loss: 0.7436683806286791\nEpoch 4500, Loss: 0.7436683806286791\nEpoch 4600, Loss: 0.7436683806286791\nEpoch 4700, Loss: 0.7436683806286791\nEpoch 4800, Loss: 0.7436683806286791\nEpoch 4900, Loss: 0.7436683806286791\nEpoch 5000, Loss: 0.7436683806286791\nEpoch 5100, Loss: 0.7436683806286791\nEpoch 5200, Loss: 0.7436683806286791\nEpoch 5300, Loss: 0.7436683806286791\nEpoch 5400, Loss: 0.7436683806286791\nEpoch 5500, Loss: 0.7436683806286791\nEpoch 5600, Loss: 0.7436683806286791\nEpoch 5700, Loss: 0.7436683806286791\nEpoch 5800, Loss: 0.7436683806286791\nEpoch 5900, Loss: 0.7436683806286791\nEpoch 6000, Loss: 0.7436683806286791\nEpoch 6100, Loss: 0.7436683806286791\nEpoch 6200, Loss: 0.7436683806286791\nEpoch 6300, Loss: 0.7436683806286791\nEpoch 6400, Loss: 0.7436683806286791\nEpoch 6500, Loss: 0.7436683806286791\nEpoch 6600, Loss: 0.7436683806286791\nEpoch 6700, Loss: 0.7436683806286791\nEpoch 6800, Loss: 0.7436683806286791\nEpoch 6900, Loss: 0.7436683806286791\nEpoch 7000, Loss: 0.7436683806286791\nEpoch 7100, Loss: 0.7436683806286791\nEpoch 7200, Loss: 0.7436683806286791\nEpoch 7300, Loss: 0.7436683806286791\nEpoch 7400, Loss: 0.7436683806286791\nEpoch 7500, Loss: 0.7436683806286791\nEpoch 7600, Loss: 0.7436683806286791\nEpoch 7700, Loss: 0.7436683806286791\nEpoch 7800, Loss: 0.7436683806286791\nEpoch 7900, Loss: 0.7436683806286791\nEpoch 8000, Loss: 0.7436683806286791\nEpoch 8100, Loss: 0.7436683806286791\nEpoch 8200, Loss: 0.7436683806286791\nEpoch 8300, Loss: 0.7436683806286791\nEpoch 8400, Loss: 0.7436683806286791\nEpoch 8500, Loss: 0.7436683806286791\nEpoch 8600, Loss: 0.7436683806286791\nEpoch 8700, Loss: 0.7436683806286791\nEpoch 8800, Loss: 0.7436683806286791\nEpoch 8900, Loss: 0.7436683806286791\nEpoch 9000, Loss: 0.7436683806286791\nEpoch 9100, Loss: 0.7436683806286791\nEpoch 9200, Loss: 0.7436683806286791\nEpoch 9300, Loss: 0.7436683806286791\nEpoch 9400, Loss: 0.7436683806286791\nEpoch 9500, Loss: 0.7436683806286791\nEpoch 9600, Loss: 0.7436683806286791\nEpoch 9700, Loss: 0.7436683806286791\nEpoch 9800, Loss: 0.7436683806286791\nEpoch 9900, Loss: 0.7436683806286791\nPredicted 4th word: Menna\n"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "d6b2c90e-d01c-4876-9679-995f8d523ceb",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}